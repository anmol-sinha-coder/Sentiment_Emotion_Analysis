{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build vocabulary and data iterator\n",
    "\n",
    "In this notebook we are going to create the vocabulary object that will be responsible for:\n",
    "- Creating dataset's vocabulary.\n",
    "- Filtering dataset in terms of the rare words occurrence and sentences lengths.\n",
    "- Mapping words to their numerical representation (word2index) and reverse (index2word).\n",
    "- Enabling the use of pre-trained word vectors.\n",
    "\n",
    "\n",
    "The second object to create is a data iterator whose task will be:\n",
    "- Sorting dataset examples.\n",
    "- Generating batches.\n",
    "- Sequence padding.\n",
    "- Enabling BatchIterator instance to iterate through all batches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin with importing all necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import torch\n",
    "from collections import defaultdict, Counter\n",
    "from pprint import pprint\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to build the vocabulary class that includes all the features mentioned at the beginning of this notebook. We want our class to enable to use of pre-trained vectors and construct the weights matrix. To be able to perform that task, we have to supply the vocabulary model with a set of pre-trained vectors.\n",
    "\n",
    "Glove vectors can be downloaded from the following website:\n",
    "https://nlp.stanford.edu/projects/glove/\n",
    "<br>\n",
    "Fasttext word vectors can be found under the link:\n",
    "https://fasttext.cc/docs/en/english-vectors.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    \n",
    "    \"\"\"The Vocab class is responsible for:\n",
    "    Creating dataset's vocabulary.\n",
    "    Filtering dataset in terms of the rare words occurrence and sentences lengths.\n",
    "    Mapping words to their numerical representation (word2index) and reverse (index2word).\n",
    "    Enabling the use of pre-trained word vectors.\n",
    "\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset : pandas.DataFrame or numpy.ndarray\n",
    "        Pandas or numpy dataset containing in the first column input strings to process and target non-string \n",
    "        variable as last column.\n",
    "    target_col: int, optional (default=None)\n",
    "        Column index refering to targets strings to process.\n",
    "    word2index: dict, optional (default=None)\n",
    "        Specify the word2index mapping.\n",
    "    sos_token: str, optional (default='<SOS>')\n",
    "        Start of sentence token.\n",
    "    eos_token: str, optional (default='<EOS>')\n",
    "        End of sentence token.\n",
    "    unk_token: str, optional (default='<UNK>')\n",
    "        Token that represents unknown words.\n",
    "    pad_token: str, optional (default='<PAD>')\n",
    "        Token that represents padding.\n",
    "    min_word_count: float, optional (default=5)\n",
    "        Specify the minimum word count threshold to include a word in vocabulary if value > 1 was passed.\n",
    "        If min_word_count <= 1 then keep all words whose count is greater than the quantile=min_word_count\n",
    "        of the count distribution.\n",
    "    max_vocab_size: int, optional (default=None)\n",
    "        Maximum size of the vocabulary.\n",
    "    max_seq_len: float, optional (default=0.8)\n",
    "        Specify the maximum length of the sequence in the dataset, if max_seq_len > 1. If max_seq_len <= 1 then set\n",
    "        the maximum length to value corresponding to quantile=max_seq_len of lengths distribution. Trimm all\n",
    "        sequences whose lengths are greater than max_seq_len.\n",
    "    use_pretrained_vectors: boolean, optional (default=False)\n",
    "        Whether to use pre-trained Glove vectors.\n",
    "    glove_path: str, optional (default='Glove/')\n",
    "        Path to the directory that contains files with the Glove word vectors.\n",
    "    glove_name: str, optional (default='glove.6B.100d.txt')\n",
    "        Name of the Glove word vectors file. Available pretrained vectors:\n",
    "        glove.6B.50d.txt\n",
    "        glove.6B.100d.txt\n",
    "        glove.6B.200d.txt\n",
    "        glove.6B.300d.txt\n",
    "        glove.twitter.27B.50d.txt\n",
    "        To use different word vectors, load their file to the vectors directory (Glove/).\n",
    "    weights_file_name: str, optional (default='Glove/weights.npy')\n",
    "        The path and the name of the numpy file to which save weights vectors.\n",
    "\n",
    "    Raises\n",
    "    -------\n",
    "    ValueError('Use min_word_count or max_vocab_size, not both!')\n",
    "        If both: min_word_count and max_vocab_size are provided.\n",
    "    FileNotFoundError\n",
    "        If the glove file doesn't exists in the given directory.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    def __init__(self, dataset, target_col=None, word2index=None, sos_token='<SOS>', eos_token='<EOS>', unk_token='<UNK>',\n",
    "             pad_token='<PAD>', min_word_count=5, max_vocab_size=None, max_seq_len=0.8,\n",
    "             use_pretrained_vectors=False, glove_path='Glove/', glove_name='glove.6B.100d.txt',\n",
    "             weights_file_name='Glove/weights.npy'):\n",
    "        \n",
    "        # Convert pandas dataframe to numpy.ndarray\n",
    "        if isinstance(dataset, pd.DataFrame):\n",
    "            dataset = dataset.to_numpy()\n",
    "        \n",
    "        self.dataset = dataset\n",
    "        self.target_col = target_col\n",
    "        \n",
    "        if self.target_col:\n",
    "            self.y_lengths = []\n",
    "            \n",
    "        self.x_lengths = []\n",
    "        self.word2idx_mapping = word2index\n",
    "        \n",
    "        # Define word2idx and idx2word as empty dictionaries\n",
    "        if self.word2idx_mapping:\n",
    "            self.word2index = self.word2idx_mapping\n",
    "        else:\n",
    "            self.word2index = defaultdict(dict)\n",
    "            self.index2word = defaultdict(dict)            \n",
    "        \n",
    "        # Instantiate special tokens\n",
    "        self.sos_token = sos_token\n",
    "        self.eos_token = eos_token\n",
    "        self.unk_token = unk_token\n",
    "        self.pad_token = pad_token\n",
    "        \n",
    "        # Instantiate min_word_count, max_vocab_size and max_seq_len\n",
    "        self.min_word_count = min_word_count\n",
    "        self.max_vocab_size = max_vocab_size\n",
    "        self.max_seq_len = max_seq_len\n",
    "        \n",
    "        self.use_pretrained_vectors = use_pretrained_vectors\n",
    "        \n",
    "        if self.use_pretrained_vectors: \n",
    "            self.glove_path = glove_path\n",
    "            self.glove_name = glove_name\n",
    "            self.weights_file_name = weights_file_name\n",
    "        \n",
    "        self.build_vocab()\n",
    "        \n",
    "        \n",
    "    def build_vocab(self):\n",
    "        \"\"\"Build the vocabulary, filter dataset sequences and create the weights matrix if specified.\n",
    "        \n",
    "        \"\"\"\n",
    "        # Create a dictionary that maps words to their count\n",
    "        self.word_count = self.word2count()\n",
    "\n",
    "        # Trim the vocabulary\n",
    "        # Get rid of out-of-vocabulary words from the dataset\n",
    "        if self.min_word_count or self.max_vocab_size:\n",
    "            self.trimVocab()\n",
    "            self.trimDatasetVocab()\n",
    "\n",
    "        # Trim sequences in terms of length\n",
    "        if self.max_seq_len:\n",
    "            if self.x_lengths:\n",
    "                self.trimSeqLen()\n",
    "\n",
    "            else:\n",
    "                # Calculate sequences lengths\n",
    "                self.x_lengths = [len(seq.split()) for seq in self.dataset[:, 0]]\n",
    "                \n",
    "                if self.target_col:\n",
    "                    self.y_lengths = [len(seq.split()) for seq in self.dataset[:, self.target_col]]\n",
    "                    \n",
    "                self.trimSeqLen()                \n",
    "\n",
    "                \n",
    "        # Map each tokens to index\n",
    "        if not self.word2idx_mapping:\n",
    "            self.mapWord2index()\n",
    "               \n",
    "        # Crate index2word mapping\n",
    "        self.index2word = {index: word for word, index in self.word2index.items()}\n",
    "        \n",
    "        # Map dataset tokens to indices\n",
    "        self.mapWords2indices()\n",
    "        \n",
    "        # Create weights matrix based on Glove vectors\n",
    "        if self.use_pretrained_vectors:\n",
    "            self.glove_vectors()       \n",
    "        \n",
    "            \n",
    "    def word2count(self):\n",
    "        \"\"\"Count the number of words occurrences.\n",
    "        \n",
    "        \"\"\"\n",
    "        # Instantiate the Counter object\n",
    "        word_count = Counter()\n",
    "\n",
    "        # Iterate through the dataset and count tokens\n",
    "        for line in self.dataset[:, 0]:\n",
    "            word_count.update(line.split())\n",
    "            \n",
    "            # Include strings from target column\n",
    "            if self.target_col:\n",
    "                for line in self.dataset[:, self.target_col]:\n",
    "                    word_count.update(line.split())\n",
    "            \n",
    "        return word_count\n",
    "    \n",
    "\n",
    "    def trimVocab(self):\n",
    "        \"\"\"Trim the vocabulary in terms of the minimum word count or the vocabulary maximum size.\n",
    "        \n",
    "        \"\"\"\n",
    "        # Trim the vocabulary in terms of the minimum word count\n",
    "        if self.min_word_count and not self.max_vocab_size:\n",
    "            # If min_word_count <= 1, use the quantile approach\n",
    "            if self.min_word_count <= 1:\n",
    "                # Create the list of words count\n",
    "                word_stat = [count for count in self.word_count.values()]\n",
    "                # Calculate the quantile of words count\n",
    "                quantile = int(np.quantile(word_stat, self.min_word_count))\n",
    "                print('Trimmed vocabulary using as mininum count threashold: quantile({:3.2f}) = {}'.\\\n",
    "                      format(self.min_word_count, quantile))\n",
    "                # Filter words using quantile threshold\n",
    "                self.trimmed_word_count = {word: count for word, count in self.word_count.items() if count >= quantile}\n",
    "            # If min_word_count > 1 use standard approach\n",
    "            else:\n",
    "                # Filter words using count threshold\n",
    "                self.trimmed_word_count = {word: count for word, count in self.word_count.items()\\\n",
    "                                   if count >= self.min_word_count}\n",
    "                print('Trimmed vocabulary using as minimum count threashold: count = {:3.2f}'.format(self.min_word_count))\n",
    "                     \n",
    "        # Trim the vocabulary in terms of its maximum size\n",
    "        elif self.max_vocab_size and not self.min_word_count:\n",
    "            self.trimmed_word_count = {word: count for word, count in self.word_count.most_common(self.max_vocab_size)}\n",
    "            print('Trimmed vocabulary using maximum size of: {}'.format(self.max_vocab_size))\n",
    "        else:\n",
    "            raise ValueError('Use min_word_count or max_vocab_size, not both!')\n",
    "            \n",
    "        print('{}/{} tokens has been retained'.format(len(self.trimmed_word_count.keys()),\n",
    "                                                     len(self.word_count.keys())))\n",
    "\n",
    "    \n",
    "    def trimDatasetVocab(self):\n",
    "        \"\"\"Get rid of rare words from the dataset sequences.\n",
    "        \n",
    "        \"\"\"\n",
    "        for row in range(self.dataset.shape[0]):\n",
    "            trimmed_x = [word for word in self.dataset[row, 0].split() if word in self.trimmed_word_count.keys()]\n",
    "            self.x_lengths.append(len(trimmed_x))\n",
    "            self.dataset[row, 0] = ' '.join(trimmed_x)\n",
    "        print('Trimmed input strings vocabulary')\n",
    "                            \n",
    "        if self.target_col:\n",
    "            for row in range(self.dataset.shape[0]):\n",
    "                trimmed_y = [word for word in self.dataset[row, self.target_col].split()\\\n",
    "                             if word in self.trimmed_word_count.keys()]\n",
    "                self.y_lengths.append(len(trimmed_y))\n",
    "                self.dataset[row, self.target_col] = ' '.join(trimmed_y)\n",
    "            print('Trimmed target strings vocabulary')\n",
    "            \n",
    "                \n",
    "    def trimSeqLen(self):\n",
    "        \"\"\"Trim dataset sequences in terms of the length.\n",
    "        \n",
    "        \"\"\"\n",
    "        if self.max_seq_len <= 1:\n",
    "            x_threshold = int(np.quantile(self.x_lengths, self.max_seq_len)) \n",
    "            if self.target_col:\n",
    "                y_threshold = int(np.quantile(self.y_lengths, self.max_seq_len)) \n",
    "        else:\n",
    "            x_threshold = self.max_seq_len\n",
    "            if self.target_col:\n",
    "                y_threshold =  self.max_seq_len\n",
    "        \n",
    "        if self.target_col:      \n",
    "            for row in range(self.dataset.shape[0]):\n",
    "                x_truncated = ' '.join(self.dataset[row, 0].split()[:x_threshold])\\\n",
    "                if self.x_lengths[row] > x_threshold else self.dataset[row, 0]\n",
    "                \n",
    "                # Add 1 if the EOS token is going to be added to the sequence\n",
    "                self.x_lengths[row] = len(x_truncated.split()) if not self.eos_token else \\\n",
    "                                      len(x_truncated.split()) + 1\n",
    "                \n",
    "                self.dataset[row, 0] = x_truncated\n",
    "                \n",
    "                y_truncated = ' '.join(self.dataset[row, self.target_col].split()[:y_threshold])\\\n",
    "                if self.y_lengths[row] > y_threshold else self.dataset[row, self.target_col]\n",
    "                \n",
    "                # Add 1 or 2 to the length to inculde special tokens\n",
    "                y_length = len(y_truncated.split())\n",
    "                if self.sos_token and not self.eos_token:\n",
    "                    y_length = len(y_truncated.split()) + 1\n",
    "                elif self.eos_token and not self.sos_token:\n",
    "                    y_length = len(y_truncated.split()) + 1\n",
    "                elif self.sos_token and self.eos_token:\n",
    "                    y_length = len(y_truncated.split()) + 2\n",
    "                    \n",
    "                self.y_lengths[row] = y_length\n",
    "                \n",
    "                self.dataset[row, self.target_col] = y_truncated\n",
    "                \n",
    "            print('Trimmed input sequences lengths to the length of: {}'.format(x_threshold))\n",
    "            print('Trimmed target sequences lengths to the length of: {}'.format(y_threshold))\n",
    "            \n",
    "        else:\n",
    "            for row in range(self.dataset.shape[0]):\n",
    "\n",
    "                x_truncated = ' '.join(self.dataset[row, 0].split()[:x_threshold])\\\n",
    "                if self.x_lengths[row] > x_threshold else self.dataset[row, 0]\n",
    "                \n",
    "                # Add 1 if the EOS token is going to be added to the sequence\n",
    "                self.x_lengths[row] = len(x_truncated.split()) if not self.eos_token else \\\n",
    "                                      len(x_truncated.split()) + 1\n",
    "                \n",
    "                self.dataset[row, 0] = x_truncated\n",
    "                \n",
    "            print('Trimmed input sequences lengths to the length of: {}'.format(x_threshold))\n",
    "                \n",
    "        \n",
    "    def mapWord2index(self):\n",
    "        \"\"\"Populate vocabulary word2index dictionary.\n",
    "        \n",
    "        \"\"\"\n",
    "        # Add special tokens as first elements in word2index dictionary\n",
    "        token_count = 0\n",
    "        for token in [self.pad_token, self.sos_token, self.eos_token, self.unk_token]:\n",
    "            if token:\n",
    "                self.word2index[token] = token_count\n",
    "                token_count += 1\n",
    "        \n",
    "        # If vocabulary is trimmed, use trimmed_word_count\n",
    "        if self.min_word_count or self.max_vocab_size:\n",
    "            for key in self.trimmed_word_count.keys():\n",
    "                self.word2index[key] = token_count\n",
    "                token_count += 1\n",
    "            \n",
    "        # If vocabulary is not trimmed, iterate through dataset    \n",
    "        else:\n",
    "            for line in self.dataset.iloc[:, 0]:\n",
    "                for word in line.split():\n",
    "                    if word not in self.word2index.keys():\n",
    "                        self.word2index[word] = token_count\n",
    "                        token_count += 1\n",
    "            # Include strings from target column\n",
    "            if self.target_col:\n",
    "                for line in self.dataset.iloc[:, self.target_col]:\n",
    "                    for word in line.split():\n",
    "                        if word not in self.word2index.keys():\n",
    "                            self.word2index[word] = token_count\n",
    "                            token_count += 1\n",
    "                            \n",
    "        self.word2index.default_factory = lambda: self.word2index[self.unk_token]\n",
    "                            \n",
    "        \n",
    "    def mapWords2indices(self):\n",
    "        \"\"\"Iterate through the dataset to map each word to its corresponding index.\n",
    "        Use special tokens if specified.\n",
    "        \n",
    "        \"\"\"\n",
    "        for row in range(self.dataset.shape[0]):\n",
    "            words2indices = []\n",
    "            for word in self.dataset[row, 0].split():\n",
    "                words2indices.append(self.word2index[word])\n",
    "                    \n",
    "            # Append the end of the sentence token\n",
    "            if self.eos_token:\n",
    "                words2indices.append(self.word2index[self.eos_token])\n",
    "                \n",
    "            self.dataset[row, 0] = np.array(words2indices)\n",
    "                \n",
    "        # Map strings from target column\n",
    "        if self.target_col:\n",
    "            for row in range(self.dataset.shape[0]):\n",
    "                words2indices = []\n",
    "                \n",
    "                # Insert the start of the sentence token\n",
    "                if self.sos_token:\n",
    "                    words2indices.append(self.word2index[self.sos_token])\n",
    "                    \n",
    "                for word in self.dataset[row, self.target_col].split():\n",
    "                    words2indices.append(self.word2index[word])\n",
    "\n",
    "                        \n",
    "                # Append the end of the sentence token\n",
    "                if self.eos_token:\n",
    "                    words2indices.append(self.word2index[self.eos_token])\n",
    "                    \n",
    "                self.dataset[row, self.target_col] = np.array(words2indices)\n",
    "           \n",
    "        print('Mapped words to indices')\n",
    "\n",
    "    \n",
    "    def glove_vectors(self):\n",
    "        \"\"\" Read glove vectors from a file, create the matrix of weights mapping vocabulary tokens to vectors.\n",
    "        Save the weights matrix to the numpy file.\n",
    "        \n",
    "        \"\"\"\n",
    "        # Load Glove word vectors to the pandas dataframe\n",
    "        try:\n",
    "            gloves = pd.read_csv(self.glove_path + self.glove_name, sep=\" \", quoting=3, header=None, index_col=0)\n",
    "        except FileNotFoundError:\n",
    "            print('File: {} not found in: {} directory'.format(self.glove_name, self.glove_path))\n",
    "            \n",
    "        # Map Glove words to vectors\n",
    "        print('Start creating glove_word2vector dictionary')\n",
    "        self.glove_word2vector = gloves.T.to_dict(orient='list')\n",
    "        \n",
    "        # Extract embedding dimension\n",
    "        emb_dim = int(re.findall('\\d+' ,self.glove_name)[-1])\n",
    "        # Length of the vocabulary\n",
    "        matrix_len = len(self.word2index)\n",
    "        # Initialize the weights matrix\n",
    "        weights_matrix = np.zeros((matrix_len, emb_dim))\n",
    "        words_found = 0\n",
    "\n",
    "        # Populate the weights matrix\n",
    "        for word, index in self.word2index.items():\n",
    "            try: \n",
    "                weights_matrix[index] = np.array(self.glove_word2vector[word])\n",
    "                words_found += 1\n",
    "            except KeyError:\n",
    "                # If vector wasn't found in Glove, initialize random vector\n",
    "                weights_matrix[index] = np.random.normal(scale=0.6, size=(emb_dim, ))\n",
    "         \n",
    "        # Save the weights matrix into numpy file\n",
    "        np.save(self.weights_file_name, weights_matrix, allow_pickle=False)\n",
    "        \n",
    "        # Delete glove_word2vector variable to free the memory\n",
    "        del self.glove_word2vector\n",
    "                        \n",
    "        print('Extracted {}/{} of pre-trained word vectors.'.format(words_found, matrix_len))\n",
    "        print('{} vectors initialized to random numbers'.format(matrix_len - words_found))\n",
    "        print('Weights vectors saved into {}'.format(self.weights_file_name))\n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the Vocab class is ready, to test its functionality, firstly we have to load the dataset that will be processed and used to build the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training set\n",
    "train_dataset = pd.read_csv('dataset/datasets_feat_clean/train_feat_clean.csv', \n",
    "                      usecols=['clean_review', 'subjectivity', 'polarity', 'word_count', 'label'],\n",
    "                      dtype={'clean_review': str, 'label': np.int16})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the columns order\n",
    "train_dataset = train_dataset[['clean_review', 'subjectivity', 'polarity', 'word_count', 'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_review</th>\n",
       "      <th>subjectivity</th>\n",
       "      <th>polarity</th>\n",
       "      <th>word_count</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>amaze good wonderful film early ninety franchi...</td>\n",
       "      <td>0.5293</td>\n",
       "      <td>0.2482</td>\n",
       "      <td>391</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>wrong end see tell chick go crazy eat old woma...</td>\n",
       "      <td>0.5750</td>\n",
       "      <td>0.1763</td>\n",
       "      <td>145</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>guess emperor clothe see list pbs night hopefu...</td>\n",
       "      <td>0.5557</td>\n",
       "      <td>0.1145</td>\n",
       "      <td>165</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>earth well movie funny sweet good plot unique ...</td>\n",
       "      <td>0.5720</td>\n",
       "      <td>0.3810</td>\n",
       "      <td>55</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>doe eye high school student kathleen beller fi...</td>\n",
       "      <td>0.4666</td>\n",
       "      <td>0.2095</td>\n",
       "      <td>688</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        clean_review  subjectivity  polarity  \\\n",
       "0  amaze good wonderful film early ninety franchi...        0.5293    0.2482   \n",
       "1  wrong end see tell chick go crazy eat old woma...        0.5750    0.1763   \n",
       "2  guess emperor clothe see list pbs night hopefu...        0.5557    0.1145   \n",
       "3  earth well movie funny sweet good plot unique ...        0.5720    0.3810   \n",
       "4  doe eye high school student kathleen beller fi...        0.4666    0.2095   \n",
       "\n",
       "   word_count  label  \n",
       "0         391      1  \n",
       "1         145      0  \n",
       "2         165      0  \n",
       "3          55      1  \n",
       "4         688      1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the first 5 rows from the dataset\n",
    "train_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we will instantiate the Vocab class, that will cause that the dataset processing begins. After it finished we will be able to access vocab attributes to check out whether all objects are created properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trimmed vocabulary using maximum size of: 5000\n",
      "5000/130416 tokens has been retained\n",
      "Trimmed input strings vocabulary\n",
      "Trimmed input sequences lengths to the length of: 121\n",
      "Mapped words to indices\n"
     ]
    }
   ],
   "source": [
    "train_vocab = Vocab(train_dataset, target_col=None, word2index=None, sos_token='<SOS>', eos_token='<EOS>',\n",
    "                    unk_token='<UNK>', pad_token='<PAD>', min_word_count=None, max_vocab_size=5000, max_seq_len=0.8,\n",
    "                    use_pretrained_vectors=False, glove_path='glove/', glove_name='glove.6B.100d.txt',\n",
    "                    weights_file_name='glove/weights.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 281,    8,  250,    5,  154, 2455,  373,  509, 1076, 1197,  796,\n",
       "        872,  328,   84,  261, 2533, 2533,   64, 1650,  499,  203,    5,\n",
       "         94,   11, 2281,  979,  796,  381, 1125,  788,  904, 4835,  979,\n",
       "         71,  392,  450, 1167,  450,  174,  979, 1497, 2958,  513,  186,\n",
       "        552, 2174, 1275,  979,  875, 1232, 4316, 2778,  617, 3143, 1233,\n",
       "        151,   72,  618,   20,  518,  712,   32, 1180,  825,  137,  206,\n",
       "       1566,   13, 2027, 1815,   57,  133,   40,  620, 1795, 1854,  253,\n",
       "       2369, 4904, 2233, 1954, 4836,  170,  361, 2174,  133,  299, 3461,\n",
       "          9,  373, 1228,  170, 2424,  273,  111,   89, 1592, 3248, 2554,\n",
       "       2778,  253, 1541, 1012,    9, 4948,   76,  337,  846,  979,  875,\n",
       "       1181,  873,  180,  316, 1012,    9,  562, 2586, 1264,  659, 1529,\n",
       "          2])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Depict the first dataset sequence\n",
    "train_vocab.dataset[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the validation set\n",
    "val_dataset = pd.read_csv('dataset/datasets_feat_clean/val_feat_clean.csv', \n",
    "                      usecols=['clean_review', 'subjectivity', 'polarity', 'word_count', 'label'],\n",
    "                      dtype={'clean_review': str, 'label': np.int16})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the columns order\n",
    "val_dataset = val_dataset[['clean_review', 'subjectivity', 'polarity', 'word_count', 'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_review</th>\n",
       "      <th>subjectivity</th>\n",
       "      <th>polarity</th>\n",
       "      <th>word_count</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>go movie twice week sum word normally use ligh...</td>\n",
       "      <td>0.4656</td>\n",
       "      <td>0.272000</td>\n",
       "      <td>155</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>year big fan park work old boy time favorite.w...</td>\n",
       "      <td>0.3833</td>\n",
       "      <td>-0.056670</td>\n",
       "      <td>119</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>movie potential handle differently need differ...</td>\n",
       "      <td>0.6560</td>\n",
       "      <td>-0.012695</td>\n",
       "      <td>152</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>movie difficult review give away plot suffice ...</td>\n",
       "      <td>0.5910</td>\n",
       "      <td>0.148400</td>\n",
       "      <td>173</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>plot worth discussion hint corruption murder p...</td>\n",
       "      <td>0.5790</td>\n",
       "      <td>0.215300</td>\n",
       "      <td>105</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        clean_review  subjectivity  polarity  \\\n",
       "0  go movie twice week sum word normally use ligh...        0.4656  0.272000   \n",
       "1  year big fan park work old boy time favorite.w...        0.3833 -0.056670   \n",
       "2  movie potential handle differently need differ...        0.6560 -0.012695   \n",
       "3  movie difficult review give away plot suffice ...        0.5910  0.148400   \n",
       "4  plot worth discussion hint corruption murder p...        0.5790  0.215300   \n",
       "\n",
       "   word_count  label  \n",
       "0         155      1  \n",
       "1         119      0  \n",
       "2         152      0  \n",
       "3         173      1  \n",
       "4         105      0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the first 5 rows from the dataset\n",
    "val_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trimmed vocabulary using maximum size of: 5000\n",
      "5000/59089 tokens has been retained\n",
      "Trimmed input strings vocabulary\n",
      "Trimmed input sequences lengths to the length of: 119\n",
      "Mapped words to indices\n"
     ]
    }
   ],
   "source": [
    "val_vocab = Vocab(val_dataset, target_col=None, word2index=train_vocab.word2index, sos_token='<SOS>', eos_token='<EOS>',\n",
    "                  unk_token='<UNK>', pad_token='<PAD>', min_word_count=None, max_vocab_size=5000, max_seq_len=0.8,\n",
    "                  use_pretrained_vectors=False, glove_path='Glove/', glove_name='glove.6B.100d.txt',\n",
    "                  weights_file_name='Glove/weights.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 387,  458,   38,   11,    5, 1011,   64, 1949,    5, 1447, 1641,\n",
       "          4,    3, 2101, 2861,   23,  585,  794,    5,   26,  292, 2768,\n",
       "         49,   62,  165,  663,   44,   31,   36,  226,   26, 1279,  809,\n",
       "          5, 3140,   62,  165,  119,   74,  132,   19, 2768, 4237, 1272,\n",
       "         50,  730,  473,    6,    5,  795, 1641,   33, 1554,   12,  685,\n",
       "        423,  222,   57,   28,  151,   23,  333,   47,    5,   77,   22,\n",
       "        809,  237, 1391,  208,  680,    7, 1094,   22,    5,   23,  605,\n",
       "         36,  423,    2])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Depict the first dataset sequence\n",
    "val_vocab.dataset[10][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next task to do is to create the BatchIterator class that will enable to sort dataset examples, generate batches of input and output variables, apply padding if required and be capable of iterating through all created batches. To warrant that the padding operation within one batch is limited, we have to sort examples within entire dataset according to sequences lengths, so that each batch will contain sequences with the most similar lengths and the number of padding tokens will be reduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchIterator:\n",
    "    \n",
    "    \"\"\"The BatchIterator class is responsible for:\n",
    "    Sorting dataset examples.\n",
    "    Generating batches.\n",
    "    Sequence padding.\n",
    "    Enabling BatchIterator instance to iterate through all batches.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset : pandas.DataFrame or numpy.ndarray\n",
    "        If vocab_created is False, pass Pandas or numpy dataset containing in the first column input strings\n",
    "        to process and target non-string variable as last column. Otherwise pass vocab.dataset object.\n",
    "    batch_size: int, optional (default=None)\n",
    "        The size of the batch. By default use batch_size equal to the dataset length.\n",
    "    vocab_created: boolean, optional (default=True)\n",
    "        Whether the vocab object is already created.\n",
    "    vocab: Vocab object, optional (default=None)\n",
    "        Use if vocab_created = True, pass the vocab object.\n",
    "    target_col: int, optional (default=None)\n",
    "        Column index refering to targets strings to process.\n",
    "    word2index: dict, optional (default=None)\n",
    "        Specify the word2index mapping.\n",
    "    sos_token: str, optional (default='<SOS>')\n",
    "        Use if vocab_created = False. Start of sentence token.\n",
    "    eos_token: str, optional (default='<EOS>')\n",
    "        Use if vocab_created = False. End of sentence token.\n",
    "    unk_token: str, optional (default='<UNK>')\n",
    "        Use if vocab_created = False. Token that represents unknown words.\n",
    "    pad_token: str, optional (default='<PAD>')\n",
    "        Use if vocab_created = False. Token that represents padding.\n",
    "    min_word_count: float, optional (default=5)\n",
    "        Use if vocab_created = False. Specify the minimum word count threshold to include a word in vocabulary\n",
    "        if value > 1 was passed. If min_word_count <= 1 then keep all words whose count is greater than the\n",
    "        quantile=min_word_count of the count distribution.\n",
    "    max_vocab_size: int, optional (default=None)\n",
    "        Use if vocab_created = False. Maximum size of the vocabulary.\n",
    "    max_seq_len: float, optional (default=0.8)\n",
    "        Use if vocab_created = False. Specify the maximum length of the sequence in the dataset, if \n",
    "        max_seq_len > 1. If max_seq_len <= 1 then set the maximum length to value corresponding to\n",
    "        quantile=max_seq_len of lengths distribution. Trimm all sequences whose lengths are greater\n",
    "        than max_seq_len.\n",
    "    use_pretrained_vectors: boolean, optional (default=False)\n",
    "        Use if vocab_created = False. Whether to use pre-trained Glove vectors.\n",
    "    glove_path: str, optional (default='Glove/')\n",
    "        Use if vocab_created = False. Path to the directory that contains files with the Glove word vectors.\n",
    "    glove_name: str, optional (default='glove.6B.100d.txt')\n",
    "        Use if vocab_created = False. Name of the Glove word vectors file. Available pretrained vectors:\n",
    "        glove.6B.50d.txt\n",
    "        glove.6B.100d.txt\n",
    "        glove.6B.200d.txt\n",
    "        glove.6B.300d.txt\n",
    "        glove.twitter.27B.50d.txt\n",
    "        To use different word vectors, load their file to the vectors directory (Glove/).\n",
    "    weights_file_name: str, optional (default='Glove/weights.npy')\n",
    "        Use if vocab_created = False. The path and the name of the numpy file to which save weights vectors.\n",
    "\n",
    "    Raises\n",
    "    -------\n",
    "    ValueError('Use min_word_count or max_vocab_size, not both!')\n",
    "        If both: min_word_count and max_vocab_size are provided.\n",
    "    FileNotFoundError\n",
    "        If the glove file doesn't exist in the given directory.\n",
    "    TypeError('Cannot convert to Tensor. Data type not recognized')\n",
    "        If the data type of the sequence cannot be converted to the Tensor.\n",
    "\n",
    "    Yields\n",
    "    ------\n",
    "    dict\n",
    "        Dictionary that contains variables batches.\n",
    "\n",
    "    \"\"\"\n",
    "        \n",
    "        \n",
    "    def __init__(self, dataset, batch_size=None, vocab_created=False, vocab=None, target_col=None, word2index=None,\n",
    "             sos_token='<SOS>', eos_token='<EOS>', unk_token='<UNK>', pad_token='<PAD>', min_word_count=5,\n",
    "             max_vocab_size=None, max_seq_len=0.8, use_pretrained_vectors=False, glove_path='Glove/',\n",
    "             glove_name='glove.6B.100d.txt', weights_file_name='Glove/weights.npy'):    \n",
    "    \n",
    "        # Create vocabulary object\n",
    "        if not vocab_created:\n",
    "            self.vocab = Vocab(dataset, target_col=target_col, word2index=word2index, sos_token=sos_token, eos_token=eos_token,\n",
    "                               unk_token=unk_token, pad_token=pad_token, min_word_count=min_word_count,\n",
    "                               max_vocab_size=max_vocab_size, max_seq_len=max_seq_len,\n",
    "                               use_pretrained_vectors=use_pretrained_vectors, glove_path=glove_path,\n",
    "                               glove_name=glove_name, weights_file_name=weights_file_name)\n",
    "            \n",
    "            # Use created vocab.dataset object\n",
    "            self.dataset = self.vocab.dataset      \n",
    "        \n",
    "        else:\n",
    "            # If vocab was created then dataset should be the vocab.dataset object\n",
    "            self.dataset = dataset\n",
    "            self.vocab = vocab\n",
    "            \n",
    "        self.target_col = target_col \n",
    "        \n",
    "        self.word2index = self.vocab.word2index\n",
    "            \n",
    "        # Define the batch_size\n",
    "        if batch_size:\n",
    "            self.batch_size = batch_size\n",
    "        else:\n",
    "            # Use the length of dataset as batch_size\n",
    "            self.batch_size = len(self.dataset)\n",
    "                \n",
    "        self.x_lengths = np.array(self.vocab.x_lengths)\n",
    "        \n",
    "        if self.target_col:\n",
    "            self.y_lengths = np.array(self.vocab.y_lengths)\n",
    "            \n",
    "        self.pad_token = self.vocab.word2index[pad_token]\n",
    "            \n",
    "        self.sort_and_batch()\n",
    "\n",
    "        \n",
    "    def sort_and_batch(self):\n",
    "        \"\"\" Sort examples within entire dataset, then perform batching and shuffle all batches.\n",
    "\n",
    "        \"\"\"\n",
    "        # Extract row indices sorted according to lengths\n",
    "        if not self.target_col:\n",
    "            sorted_indices = np.argsort(self.x_lengths)\n",
    "        else:\n",
    "            sorted_indices = np.lexsort((self.y_lengths, self.x_lengths))\n",
    "        \n",
    "        # Sort all sets\n",
    "        self.sorted_dataset = self.dataset[sorted_indices[::-1]]\n",
    "        self.sorted_x_lengths = np.flip(self.x_lengths[sorted_indices])\n",
    "        \n",
    "        if self.target_col:\n",
    "            self.sorted_target = self.sorted_dataset[:, self.target_col]\n",
    "            self.sorted_y_lengths = np.flip(self.x_lengths[sorted_indices])\n",
    "        else:\n",
    "            self.sorted_target = self.sorted_dataset[:, -1]\n",
    "        \n",
    "        # Initialize input, target and lengths batches\n",
    "        self.input_batches = [[] for _ in range(self.sorted_dataset.shape[1]-1)]\n",
    "        \n",
    "        self.target_batches, self.x_len_batches = [], []\n",
    "\n",
    "        self.y_len_batches = [] if self.target_col else None\n",
    "        \n",
    "        # Create batches\n",
    "        for i in range(self.sorted_dataset.shape[1]-1):\n",
    "            # The first column contains always sequences that should be padded.\n",
    "            if i == 0:\n",
    "                self.create_batches(self.sorted_dataset[:, i], self.input_batches[i], pad_token=self.pad_token)\n",
    "            else:\n",
    "                self.create_batches(self.sorted_dataset[:, i], self.input_batches[i])\n",
    "                \n",
    "        if self.target_col:\n",
    "            self.create_batches(self.sorted_target, self.target_batches, pad_token=self.pad_token)\n",
    "            self.create_batches(self.sorted_y_lengths, self.y_len_batches)\n",
    "        else:\n",
    "            self.create_batches(self.sorted_target, self.target_batches)\n",
    "        \n",
    "        self.create_batches(self.sorted_x_lengths, self.x_len_batches)\n",
    "        \n",
    "        # Shuffle batches\n",
    "        self.indices = np.arange(len(self.input_batches[0]))\n",
    "        np.random.shuffle(self.indices)\n",
    "        \n",
    "        for j in range(self.sorted_dataset.shape[1]-1):\n",
    "            self.input_batches[j] = [self.input_batches[j][i] for i in self.indices]\n",
    "        \n",
    "        self.target_batches = [self.target_batches[i] for i in self.indices]\n",
    "        self.x_len_batches = [self.x_len_batches[i] for i in self.indices]\n",
    "        \n",
    "        if self.target_col:\n",
    "            self.y_len_batches = [self.y_len_batches[i] for i in self.indices]\n",
    "        \n",
    "        print('Batches created')\n",
    "        \n",
    "        \n",
    "    def create_batches(self, sorted_dataset, batches, pad_token=-1):\n",
    "        \"\"\" Convert each sequence to pytorch Tensor, create batches and pad them if required.\n",
    "        \n",
    "        \"\"\"\n",
    "        # Calculate the number of batches\n",
    "        n_batches = int(len(sorted_dataset)/self.batch_size)\n",
    "\n",
    "        # Create list of batches\n",
    "        list_of_batches = np.array([sorted_dataset[i*self.batch_size:(i+1)*self.batch_size].copy()\\\n",
    "                                    for i in range(n_batches+1)])\n",
    "\n",
    "        # Convert each sequence to pytorch Tensor\n",
    "        for batch in list_of_batches:\n",
    "            tensor_batch = []\n",
    "            tensor_type = None\n",
    "            for seq in batch:\n",
    "                # Check seq data type and convert to Tensor\n",
    "                if isinstance(seq, np.ndarray):\n",
    "                    tensor = torch.LongTensor(seq)\n",
    "                    tensor_type = 'int'\n",
    "                elif isinstance(seq, np.integer):\n",
    "                    tensor = torch.LongTensor([seq])\n",
    "                    tensor_type = 'int'\n",
    "                elif isinstance(seq, np.float):\n",
    "                    tensor = torch.FloatTensor([seq])\n",
    "                    tensor_type = 'float'\n",
    "                elif isinstance(seq, int):\n",
    "                    tensor = torch.LongTensor([seq])\n",
    "                    tensor_type = 'int'\n",
    "                elif isinstance(seq, float):\n",
    "                    tensor = torch.FloatTensor([seq])\n",
    "                    tensor_type = 'float'\n",
    "                else:\n",
    "                    raise TypeError('Cannot convert to Tensor. Data type not recognized')\n",
    "\n",
    "                tensor_batch.append(tensor)\n",
    "            if pad_token != -1:\n",
    "                # Pad required sequences\n",
    "                pad_batch = torch.nn.utils.rnn.pad_sequence(tensor_batch, batch_first=True)\n",
    "                batches.append(pad_batch)\n",
    "            else:\n",
    "                if tensor_type == 'int':\n",
    "                    batches.append(torch.LongTensor(tensor_batch))\n",
    "                else:\n",
    "                    batches.append(torch.FloatTensor(tensor_batch))\n",
    "\n",
    "                \n",
    "    def __iter__(self):\n",
    "        \"\"\" Iterate through batches.\n",
    "        \n",
    "        \"\"\"\n",
    "        # Create a dictionary that holds variables batches to yield\n",
    "        to_yield = {}\n",
    "        \n",
    "        # Iterate through batches\n",
    "        for i in range(len(self.input_batches[0])):\n",
    "            feat_list = []\n",
    "            for j in range(1, len(self.input_batches)):\n",
    "                feat = self.input_batches[j][i].type(torch.FloatTensor).unsqueeze(1)\n",
    "                feat_list.append(feat)\n",
    "                \n",
    "            if feat_list:\n",
    "                input_feat = torch.cat(feat_list, dim=1)\n",
    "                to_yield['input_feat'] = input_feat\n",
    "\n",
    "            to_yield['input_seq'] = self.input_batches[0][i]\n",
    "\n",
    "            to_yield['target'] = self.target_batches[i]\n",
    "            to_yield['x_lengths'] = self.x_len_batches[i]\n",
    "            \n",
    "            if self.target_col:\n",
    "                to_yield['y_length'] = self.y_len_batches[i]\n",
    "\n",
    "\n",
    "            yield to_yield\n",
    "            \n",
    "            \n",
    "    def __len__(self):\n",
    "        \"\"\" Return iterator length.\n",
    "        \n",
    "        \"\"\"\n",
    "        return len(self.input_batches[0])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to instantiate the BatchIterator class and check out whether all tasks were conducted correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trimmed vocabulary using as minimum count threashold: count = 5.00\n",
      "26346/130416 tokens has been retained\n",
      "Trimmed input strings vocabulary\n",
      "Trimmed input sequences lengths to the length of: 138\n",
      "Mapped words to indices\n",
      "Batches created\n"
     ]
    }
   ],
   "source": [
    "train_iterator = BatchIterator(train_dataset, batch_size=32, vocab_created=False, vocab=None, target_col=None,\n",
    "                               word2index=None, sos_token='<SOS>', eos_token='<EOS>', unk_token='<UNK>',\n",
    "                               pad_token='<PAD>', min_word_count=5, max_vocab_size=None, max_seq_len=0.8,\n",
    "                               use_pretrained_vectors=False, glove_path='glove/', glove_name='glove.6B.100d.txt',\n",
    "                               weights_file_name='glove/weights.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the size of first input batch\n",
    "len(train_iterator.input_batches[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_feat': tensor([[ 4.7220e-01,  2.0750e-01,  1.4900e+02],\n",
      "        [ 7.1240e-01,  5.4000e-02,  1.5900e+02],\n",
      "        [ 5.4540e-01,  1.6820e-01,  1.1700e+02],\n",
      "        [ 3.3100e-01,  6.5600e-02,  1.7800e+02],\n",
      "        [ 5.8800e-01,  2.6560e-01,  1.8600e+02],\n",
      "        [ 5.8200e-01,  4.8900e-01,  1.5200e+02],\n",
      "        [ 6.7720e-01,  2.3170e-01,  1.6200e+02],\n",
      "        [ 5.7960e-01,  1.9180e-01,  1.5000e+02],\n",
      "        [ 5.1700e-01,  8.3400e-02,  1.4300e+02],\n",
      "        [ 4.0970e-01,  1.5230e-01,  1.4600e+02],\n",
      "        [ 4.7780e-01,  9.1700e-02,  1.2500e+02],\n",
      "        [ 5.1300e-01, -1.8940e-02,  1.5100e+02],\n",
      "        [ 7.0300e-01,  3.5280e-01,  1.6800e+02],\n",
      "        [ 5.1100e-01,  7.5000e-02,  1.4000e+02],\n",
      "        [ 6.0800e-01, -1.7430e-01,  1.5900e+02],\n",
      "        [ 5.4700e-01,  1.6260e-01,  1.3800e+02],\n",
      "        [ 5.7860e-01,  1.4540e-01,  1.4000e+02],\n",
      "        [ 4.5120e-01,  1.9440e-02,  1.6500e+02],\n",
      "        [ 5.1200e-01, -1.5480e-01,  1.3600e+02],\n",
      "        [ 5.3370e-01,  3.3500e-01,  1.2700e+02],\n",
      "        [ 3.9060e-01,  1.1630e-01,  1.3300e+02],\n",
      "        [ 5.1950e-01,  1.8520e-01,  1.6800e+02],\n",
      "        [ 5.1560e-01,  2.0080e-01,  1.8100e+02],\n",
      "        [ 6.7900e-01, -5.2400e-03,  1.3800e+02],\n",
      "        [ 6.0640e-01, -5.4170e-02,  1.5300e+02],\n",
      "        [ 5.3860e-01,  7.9900e-02,  1.5700e+02],\n",
      "        [ 4.7240e-01,  1.1230e-01,  1.2000e+02],\n",
      "        [ 5.9130e-01,  1.0360e-01,  1.5800e+02],\n",
      "        [ 4.4200e-01,  1.0450e-01,  1.9500e+02],\n",
      "        [ 4.5360e-01,  1.7600e-02,  1.3100e+02],\n",
      "        [ 6.5140e-01,  2.1410e-01,  1.5500e+02],\n",
      "        [ 5.6600e-01,  1.1604e-02,  1.6000e+02]]),\n",
      " 'input_seq': tensor([[  173,   672,   564,  ...,   564,  3192,     2],\n",
      "        [  142,     5,  4233,  ...,   245,   988,     2],\n",
      "        [  396,   463,  1105,  ...,    21,   364,     2],\n",
      "        ...,\n",
      "        [23122,   170,   419,  ...,   829,     7,     2],\n",
      "        [10061,   681,  1135,  ..., 10061,   833,     2],\n",
      "        [ 2823, 11805,   424,  ...,  1680,  1206,     2]]),\n",
      " 'target': tensor([0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,\n",
      "        0, 0, 1, 0, 0, 1, 1, 0]),\n",
      " 'x_lengths': tensor([64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64,\n",
      "        64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64])}\n"
     ]
    }
   ],
   "source": [
    "# Run the BatchIterator and print the first set of batches\n",
    "for batches in train_iterator:\n",
    "    pprint(batches)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trimmed vocabulary using as minimum count threashold: count = 5.00\n",
      "14097/59089 tokens has been retained\n",
      "Trimmed input strings vocabulary\n",
      "Trimmed input sequences lengths to the length of: 132\n",
      "Mapped words to indices\n",
      "Batches created\n"
     ]
    }
   ],
   "source": [
    "val_iterator = BatchIterator(val_dataset, batch_size=32, vocab_created=False, vocab=None, target_col=None,\n",
    "                             word2index=train_iterator.word2index, sos_token='<SOS>', eos_token='<EOS>',\n",
    "                             unk_token='<UNK>', pad_token='<PAD>', min_word_count=5, max_vocab_size=None,\n",
    "                             max_seq_len=0.8, use_pretrained_vectors=False, glove_path='glove/',\n",
    "                             glove_name='glove.6B.100d.txt', weights_file_name='glove/weights.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_feat': tensor([[ 3.2200e-01, -2.8530e-03,  3.0100e+02],\n",
      "        [ 5.9960e-01, -1.6210e-01,  3.5000e+02],\n",
      "        [ 6.0300e-01,  2.5220e-01,  3.8500e+02],\n",
      "        [ 4.9660e-01, -5.3330e-03,  4.3700e+02],\n",
      "        [ 4.9800e-01,  1.9060e-01,  5.7500e+02],\n",
      "        [ 4.3650e-01,  2.8100e-01,  5.1800e+02],\n",
      "        [ 6.4650e-01,  2.6880e-01,  4.6200e+02],\n",
      "        [ 5.4150e-01, -8.0800e-02,  6.7300e+02],\n",
      "        [ 4.5700e-01,  1.5900e-01,  4.0800e+02],\n",
      "        [ 5.2440e-01, -1.0614e-01,  5.1800e+02],\n",
      "        [ 5.2340e-01,  9.8800e-03,  3.5600e+02],\n",
      "        [ 4.8400e-01,  2.2500e-01,  2.9900e+02],\n",
      "        [ 6.1700e-01,  1.4420e-01,  3.3600e+02],\n",
      "        [ 4.4530e-01, -9.2700e-03,  4.2600e+02],\n",
      "        [ 4.0430e-01,  2.9110e-02,  2.7400e+02],\n",
      "        [ 5.3400e-01,  1.8950e-01,  5.0700e+02],\n",
      "        [ 5.5600e-01,  6.7570e-02,  3.5900e+02],\n",
      "        [ 6.6000e-01,  1.6110e-01,  4.5400e+02],\n",
      "        [ 5.7100e-01,  1.2940e-01,  5.1000e+02],\n",
      "        [ 6.1670e-01,  5.5760e-02,  7.1400e+02],\n",
      "        [ 6.3300e-01, -1.1725e-01,  4.3000e+02],\n",
      "        [ 5.2050e-01,  8.1670e-02,  2.6700e+02],\n",
      "        [ 4.5750e-01, -1.2463e-01,  3.8800e+02],\n",
      "        [ 4.0330e-01,  1.9690e-01,  2.8800e+02],\n",
      "        [ 4.2300e-01,  7.3100e-02,  3.9000e+02],\n",
      "        [ 6.1000e-01,  3.6670e-01,  3.4900e+02],\n",
      "        [ 5.9770e-01,  2.0150e-01,  9.9500e+02],\n",
      "        [ 4.9150e-01, -1.2410e-02,  6.7400e+02],\n",
      "        [ 4.8240e-01,  1.0414e-02,  4.7700e+02],\n",
      "        [ 4.8120e-01,  3.9980e-02,  6.8600e+02],\n",
      "        [ 4.4650e-01,  8.8870e-02,  3.5800e+02],\n",
      "        [ 4.7310e-01,  5.6500e-02,  7.8800e+02]]),\n",
      " 'input_seq': tensor([[20002,  1442,   142,  ...,  1594,  4945,     2],\n",
      "        [ 3274,   389,   185,  ...,  4246,   923,     2],\n",
      "        [ 1835,  3605,  8234,  ...,   153,  3515,     2],\n",
      "        ...,\n",
      "        [24293,   135,   173,  ...,    80,  2637,     2],\n",
      "        [  801,    51,  4351,  ...,   490,   138,     2],\n",
      "        [    7,   413, 11481,  ...,  5524,  1589,     2]]),\n",
      " 'target': tensor([1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1,\n",
      "        0, 1, 1, 0, 0, 1, 0, 0]),\n",
      " 'x_lengths': tensor([133, 133, 133, 133, 133, 133, 133, 133, 133, 133, 133, 133, 133, 133,\n",
      "        133, 133, 133, 133, 133, 133, 133, 133, 133, 133, 133, 133, 133, 133,\n",
      "        133, 133, 133, 133])}\n"
     ]
    }
   ],
   "source": [
    "# Run the BatchIterator and print the first set of batches\n",
    "for batches in val_iterator:\n",
    "    pprint(batches)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next notebook we are going to create the neural network model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
