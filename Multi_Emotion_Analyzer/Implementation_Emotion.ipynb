{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Implementation_Emotion.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOkDwzAPR6m4PjNJrsqXvKH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anmol-sinha-coder/Sentiment_Emotion_Analysis/blob/main/Multi_Emotion_Analyzer/Implementation_Emotion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NAg-V9kM22sx",
        "outputId": "c21e2338-69d8-44de-9633-3fee62560ac8"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/Google_Drive\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/Google_Drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! wget https://worksheets.codalab.org/rest/bundles/0x97c870dd60eb4f0fa53f257978851c60/contents/blob/glove.6B.200d.txt -P data/\n",
        "# ! wget https://github.com/anmol-sinha-coder/Sentiment_Emotion_Analysis/raw/main/Multi_Emotion_Analyzer/emotion_model.pth\n",
        "! cp -ra Google_Drive/MyDrive/ADNN/emotion_model.pth /content/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BLdXsuo7sNmX",
        "outputId": "7613b63f-848f-4ba2-e36f-5b5f553d4e7d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-02-12 07:24:31--  https://worksheets.codalab.org/rest/bundles/0x97c870dd60eb4f0fa53f257978851c60/contents/blob/glove.6B.200d.txt\n",
            "Resolving worksheets.codalab.org (worksheets.codalab.org)... 13.68.212.115\n",
            "Connecting to worksheets.codalab.org (worksheets.codalab.org)|13.68.212.115|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Syntax error in Set-Cookie: codalab_session=\"\"; expires=Thu, 01 Jan 1970 00:00:00 GMT; Max-Age=-1; Path=/ at position 70.\n",
            "Length: unspecified [text/plain]\n",
            "Saving to: ‘data/glove.6B.200d.txt’\n",
            "\n",
            "glove.6B.200d.txt       [       <=>          ] 661.31M  47.0MB/s    in 15s     \n",
            "\n",
            "2022-02-12 07:24:46 (44.3 MB/s) - ‘data/glove.6B.200d.txt’ saved [693432828]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3dh3PviD4YU"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "As_dueg0KP45"
      },
      "source": [
        "def read_glove_vecs(glove_file):\n",
        "    with open(glove_file, 'r') as f:\n",
        "        words = set()\n",
        "        word_to_vec_map = {}\n",
        "        for line in f:\n",
        "            line = line.strip().split()\n",
        "            curr_word = line[0]\n",
        "            words.add(curr_word)\n",
        "            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n",
        "        \n",
        "        i = 1\n",
        "        words_to_index = {}\n",
        "        index_to_words = {}\n",
        "        for w in sorted(words):\n",
        "            words_to_index[w] = i\n",
        "            index_to_words[i] = w\n",
        "            i = i + 1\n",
        "    return words_to_index, index_to_words, word_to_vec_map\n",
        "\n",
        "\n",
        "\n",
        "def pretrained_embedding_layer(word_to_vec_map, word_to_index, non_trainable=True):\n",
        "    num_embeddings = len(word_to_index) + 1                   \n",
        "    embedding_dim = word_to_vec_map[\"cucumber\"].shape[0]  #  dimensionality of GloVe word vectors (= 50)\n",
        "\n",
        "    # Initialize the embedding matrix as a numpy array of zeros of shape (num_embeddings, embedding_dim)\n",
        "    weights_matrix = np.zeros((num_embeddings, embedding_dim))\n",
        "\n",
        "    # Set each row \"index\" of the embedding matrix to be the word vector representation of the \"index\"th word of the vocabulary\n",
        "    for word, index in word_to_index.items():\n",
        "        weights_matrix[index, :] = word_to_vec_map[word]\n",
        "\n",
        "    embed = nn.Embedding.from_pretrained(torch.from_numpy(weights_matrix).type(torch.FloatTensor), freeze=non_trainable)\n",
        "\n",
        "    return embed, num_embeddings, embedding_dim\n",
        "\n",
        "\n",
        "\n",
        "def sentences_to_indices(X, word_to_index, max_len):\n",
        "    \"\"\"\n",
        "    Converts an array of sentences (strings) into an array of indices corresponding to words in the sentences.\n",
        "    \"\"\"\n",
        "    m = X.shape[0]  # number of training examples\n",
        "    \n",
        "    # Initialize X_indices as a numpy matrix of zeros and the correct shape\n",
        "    X_indices = np.zeros((m,max_len))\n",
        "    \n",
        "    for i in range(m):  # loop over training examples\n",
        "        \n",
        "        # Convert the ith sentence in lower case and split into a list of words\n",
        "        sentence_words = X[i].lower().split()\n",
        "        \n",
        "        # Initialize j to 0\n",
        "        j = 0\n",
        "        \n",
        "        # Loop over the words of sentence_words\n",
        "        for w in sentence_words:\n",
        "            # Set the (i,j)th entry of X_indices to the index of the correct word.\n",
        "            X_indices[i, j] = word_to_index[w]\n",
        "            # Increment j to j + 1\n",
        "            j = j + 1\n",
        "    \n",
        "    return X_indices\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3UDW8vEkMuhy"
      },
      "source": [
        "#X_train, Y_train = read_csv('/content/data/train.csv')\n",
        "#X_test, Y_test = read_csv('/content/data/test.csv')\n",
        "word_to_index, index_to_word, word_to_vec_map = read_glove_vecs('/content/data/glove.6B.200d.txt')\n",
        "embedding, vocab_size, embedding_dim = pretrained_embedding_layer(word_to_vec_map, word_to_index, non_trainable=True)\n",
        "\n",
        "hidden_dim=128\n",
        "output_size=5\n",
        "batch_size = 32\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0IXKdvk6FrG"
      },
      "source": [
        "class NN(nn.Module):\n",
        "  def __init__(self, embedding, embedding_dim, hidden_dim, vocab_size, output_dim, batch_size):\n",
        "      super(NN, self).__init__()\n",
        "\n",
        "      self.batch_size = batch_size\n",
        "\n",
        "      self.hidden_dim = hidden_dim\n",
        "\n",
        "      self.word_embeddings = embedding\n",
        "\n",
        "      # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
        "      # with dimensionality hidden_dim.\n",
        "      self.lstm = nn.LSTM(embedding_dim, \n",
        "                          hidden_dim, \n",
        "                          num_layers=2,\n",
        "                          dropout = 0.5,\n",
        "                          batch_first = True)\n",
        "\n",
        "      # The linear layer that maps from hidden state space to output space\n",
        "      self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "  def forward(self, sentence):\n",
        "      \n",
        "      #sentence = sentence.type(torch.LongTensor)\n",
        "      #print ('Shape of sentence is:', sentence.shape)\n",
        "\n",
        "      sentence = sentence.to(device)\n",
        "\n",
        "      embeds = self.word_embeddings(sentence)\n",
        "      #print ('Embedding layer output shape', embeds.shape)\n",
        "\n",
        "      # initializing the hidden state to 0\n",
        "      #hidden=None\n",
        "      \n",
        "      h0 = torch.zeros(2, sentence.size(0), hidden_dim).requires_grad_().to(device)\n",
        "      c0 = torch.zeros(2, sentence.size(0), hidden_dim).requires_grad_().to(device)\n",
        "      \n",
        "      lstm_out, h = self.lstm(embeds, (h0, c0))\n",
        "      # get info from last timestep only\n",
        "      lstm_out = lstm_out[:, -1, :]\n",
        "      #print ('LSTM layer output shape', lstm_out.shape)\n",
        "      #print ('LSTM layer output ', lstm_out)\n",
        "\n",
        "      # Dropout\n",
        "      lstm_out = F.dropout(lstm_out, 0.5)\n",
        "\n",
        "      fc_out = self.fc(lstm_out)\n",
        "      #print ('FC layer output shape', fc_out.shape)\n",
        "      #print ('FC layer output ', fc_out)\n",
        "      \n",
        "      out = fc_out\n",
        "      out = F.softmax(out, dim=1)\n",
        "      #print ('Output layer output shape', out.shape)\n",
        "      #print ('Output layer output ', out)\n",
        "      return out"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1RmTs10r6m5o",
        "outputId": "6bcb59de-6308-4505-cc37-a67e7c93b92b"
      },
      "source": [
        "model_emotions = NN(embedding, embedding_dim, hidden_dim, vocab_size, output_size, batch_size).to(device)\n",
        "model_emotions.load_state_dict(torch.load('emotion_model.pth', map_location=device))\n",
        "model_emotions.eval()\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NN(\n",
              "  (word_embeddings): Embedding(400001, 200)\n",
              "  (lstm): LSTM(200, 128, num_layers=2, batch_first=True, dropout=0.5)\n",
              "  (fc): Linear(in_features=128, out_features=5, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-IWGiFSA2Fs"
      },
      "source": [
        "def predict(input_text, print_sentence=True):\n",
        "  labels_dict = {\n",
        "\t\t0 : \"❤️ Loving\",\n",
        "\t\t1 : \"⚽️ Playful/Excited\",\n",
        "\t\t2 : \"😄 Happy\",\n",
        "\t\t3 : \"😞 Annoyed/Sad\",\n",
        "\t\t4 : \"🍽 Foodie\",\n",
        "\t}\n",
        "  \n",
        "  # Convert the input to the model\n",
        "  x_test = np.array([input_text])\n",
        "  maxLen = len(max(np.asarray([input_text]), key=len).split())\n",
        "  X_test_indices = sentences_to_indices(x_test, word_to_index, maxLen)\n",
        "  sentences = torch.tensor(X_test_indices).type(torch.LongTensor)\n",
        "  sentences\n",
        "  model_emotions.forward(sentences)\n",
        "  # Get the class label\n",
        "  ps = model_emotions(sentences)\n",
        "\n",
        "  top_p, top_class = ps.topk(1, dim=1)\n",
        "  label = int(top_class[0][0])\n",
        "\n",
        "  if print_sentence:\n",
        "    print(\"\\nInput Text: \\t\"+ input_text +'\\nEmotion: \\t'+  labels_dict[label])\n",
        "\n",
        "  return label"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNG0PHKEQbtn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9f86c0a-05a0-45bb-a363-ad607f047512"
      },
      "source": [
        "predict(\"This is the best day of my life\")\n",
        "predict(\"What the hell are you doing ?\")\n",
        "predict(\"Vishwanathan Anand won in chess !\")\n",
        "predict(\"She really likes you now\")\n",
        "predict(\"I completely hate her\")\n",
        "predict(\"The food smells nice\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Input Text: \tThis is the best day of my life\n",
            "Emotion: \t😄 Happy\n",
            "\n",
            "Input Text: \tWhat the hell are you doing ?\n",
            "Emotion: \t😞 Annoyed/Sad\n",
            "\n",
            "Input Text: \tVishwanathan Anand won in chess !\n",
            "Emotion: \t⚽️ Playful/Excited\n",
            "\n",
            "Input Text: \tShe really likes you now\n",
            "Emotion: \t❤️ Loving\n",
            "\n",
            "Input Text: \tI completely hate her\n",
            "Emotion: \t😞 Annoyed/Sad\n",
            "\n",
            "Input Text: \tThe food smells nice\n",
            "Emotion: \t🍽 Foodie\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lemmatize, Tokenize, Stemming for Natural Language Processing\n",
        "<hr/>\n",
        "\n",
        "<img src=\"https://static.javatpoint.com/tutorial/nlp/images/phases-of-nlp.png\">"
      ],
      "metadata": {
        "id": "O9NAooge15zm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk"
      ],
      "metadata": {
        "id": "0Rr7R94u2kNA"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCMsBE3GRM6N"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}